---
title: "Assignment - Midterm - Boston Housing"
subtitle: "Multivariate Analysis"
author: "Ignacio Almodóvar, Luis Rodríguez, Javier Muñoz"
date: "12/12/2021"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{mathtools}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First of all we are going to load all the libraries needed for the analysis and read the Boston house prices dataset file.

```{r,warning=FALSE}
library(ggplot2)
library(dplyr)
library(MASS)
library(RColorBrewer)
library(GGally)
library(andrews)
library(corrplot)
library(rrcov)
library(mice)

dataset <- read.csv("Dataset.csv")
```

## Variables explanation

This dataset has been taken from [Kaggle](https://www.kaggle.com/c/boston-housing) and it contains information about housing market suburbs in Boston. It contains 511 observations which represent different suburbs in Boston area. It is formed by 14 variables, which are:

- _CRIM_: This is a continuous variable that measures the crime per capital rate by town.
- _ZN_: Continuous variable that indicates the proportion of residential land zoned for lots over 25,000 sq.ft. There are not many different values in this category. Could be also considered a categorical variable.
- _INDUS_: Proportion of non-retail business acres per town. Continuous variable.
- _CHAS_: It is a binary variable referred to Charles River (= 1 if tract bounds river; 0 otherwise).
- _NOX_: Continuous variable that measures nitric oxides concentration (parts per 10 million).
- _RM_: Measures the average number of rooms per dwelling. Continuous variable.
- _AGE_: Continuous variable that measures the proportion of owner-occupied units built prior to 1940.
- _DIS_: Continuous variable that indicates the weighted distances to five Boston employment centers.
- _RAD_: Index of accessibility to radial highways. Larger index denotes better accessibility.
- _TAX_: Continuous variable full-value property-tax rate per $10,000.
- _PTRATIO_: Continuous variable that measures the proportion of pupil-teacher by town.
- _B_: Is a continuous variable based on the formula 1000(Bk - 0.63)^2, where BK is the proportion of black people.
- _LSTAT_: Continuous variable that measures the proportion of adults without, some high school education and proportion of male workers classified as laborers.
- _MEDV_: Continuous variable with the median value price of owner-occupied homes in $1000's

Analyzing the different values taken by the variables we noticed that some of them can be categorized. Therefore, we have created some categorical variable from the original data to summarize this information:


- _RAD.CAT_: low, medium and high
- _RM.CAT_: low, medium and high
- _INDUS.CAT_: very low, low, high and very high
- _LSTAT.CAT_: Low-class, Medium-High-class
- _ZN.CAT_: very low, low, high and very high

```{r}
dataset.with.categories <- dataset

#Convert to factor
dataset.with.categories$CHAS <- factor(dataset.with.categories$CHAS)

factor_rad_values <- sort(unique(dataset.with.categories$RAD))
delimiter_rad_index <- length(factor_rad_values)/3
lower_delimiter_rad_value <- factor_rad_values[round(delimiter_rad_index)]
upper_delimiter_rad_value <- factor_rad_values[round(2*delimiter_rad_index)]

dataset.with.categories$RAD.CAT <- cut(dataset.with.categories$RAD, 
    breaks = c(-Inf, lower_delimiter_rad_value, upper_delimiter_rad_value, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

dataset.with.categories$RM.CAT <- cut(dataset.with.categories$RM, 
    breaks = c(-Inf, 4, 6, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

factor_indus_values <- sort(unique(dataset.with.categories$INDUS))
delimiter_indus_index <- length(factor_indus_values)/4
lower_delimiter_indus_value <- factor_indus_values[round(delimiter_indus_index)]
mid_delimiter_indus_value <- factor_indus_values[round(2*delimiter_indus_index)]
upper_delimiter_indus_value <- factor_indus_values[round(3*delimiter_indus_index)]

dataset.with.categories$INDUS.CAT <- cut(dataset.with.categories$INDUS, 
    breaks = c(-Inf, lower_delimiter_indus_value, mid_delimiter_indus_value, upper_delimiter_indus_value, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

dataset.with.categories$LSTAT.CAT <- cut(dataset.with.categories$LSTAT, 
    breaks = c(-Inf, 25, Inf), 
    labels = c('Medium-High-class', 'Low-class'),
    right = FALSE)

dataset.with.categories$ZN.CAT <- cut(dataset.with.categories$ZN, 
    breaks = c(-Inf, 25, 50, 75, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

str(dataset.with.categories)

```

In order to visualize all the data and the different relations that it has, we are going to define a method to generate the kernel plots for continuous variables for all the groups in each category.

```{r}
kernel_by_factors <- function(x, x_name, type_color) {
  factors <- Filter(is.factor, dataset.with.categories)
  num_vars <- ncol(factors)
  mod <- num_vars %% 2 
  
  colors = brewer.pal(n = num_vars + 1, name=type_color)

  par(mfrow=c(num_vars+mod+1,2), mar=c(2,2,2,2))
  plot(density(x,kernel="gaussian"),
   ylab="Density",
   main=paste0("Kernel density of ", x_name),
   xlab=x_name,
   col=colors[1],
   lwd=5)
  
  index_factor <- 1
  for(factor in factors) {
    plot_kernel_charts(x, factor[!is.na(factor)], x_name, names(factors[index_factor]), colors)
    index_factor = index_factor + 1
  }
}

plot_kernel_charts <- function(x, cat, x_name, cat_name, colors) {
  cat_values <- levels(cat)
  num_colors <- length(cat_values)
  
  min_x <<- c()
  max_x <<- c()
  min_y <<- c()
  max_y <<- c()
  
  densities <<- list()
  
  for(cat_value in cat_values)
    generate_kernel_by_cat_value(x, cat, cat_value)
  
  min_x <- min(min_x)
  max_x <- max(max_x)
  min_y <- min(min_y)
  max_y <- max(max_y)
  
  plot(c(min_x,max_x),c(min_y,max_y),
       xlab=x_name,
       ylab="Density",
       main=paste0("Kernel: ", x_name, " in terms of ", cat_name),
       type="n")
  legend(x="topright", legend=c(cat_values),col=colors[-1], lty=1, cex=0.8,
     box.lty=0)
  i <- 2
  for(densitiy_by_cat in densities) {
    lines(densitiy_by_cat$x, densitiy_by_cat$y, col=colors[i], lwd=1)
    i = i + 1
  }
}

generate_kernel_by_cat_value <- function(x, cat, cat_value){
  if(length(cat[cat==cat_value]) > 2){
    d_cat_value <- density(x[cat==cat_value], kernel="gaussian")
    min_x <<- c(min_x, min(d_cat_value$x))
    max_x <<- c(max_x, max(d_cat_value$x))
    min_y <<- c(min_y, min(d_cat_value$y))
    max_y <<- c(max_y, max(d_cat_value$y))
    
    densities <<- append(densities, list(d_cat_value))
  }
}

```

Now we are going to use these functions to plot different continuous variables in order to get knowladge about the different groups.

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$TAX, "TAX", "Set1")
```

Within the first plot we can see how the variable TAX is distributed. It follows a bi-modal distribution, which means that there are two trends for the taxes paid. Notice that for high values on taxes, there is only one group that provides the weight for this second trend.  In particular, it happens with the plots associated to "RAD.CAT","INDUS.CAT" and "ZN.CAT". 

Now, let's analyze the median price of the houses.

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$MEDV, "MEDV", "Dark2")
```

We can see that for the median value of houses from different suburbs, it is difficult to distinguish distributions for different groups. However, we can see a difference in the variable LSTAT.CAT, as the density curves are significantly different for both classes in terms of skewness.

```{r}
ggplot(data=dataset.with.categories,aes(y=MEDV,x=LSTAT))+geom_point(aes(colour=LSTAT.CAT))
```

Within the scatter plot we can easily visualize the difference of populations. The lower percentage of people considered low-class the higher the price of houses.

For the CRIM variable we are going to apply logarithm in order to obtain a better visualization of the plot, due to the strong skewness.

```{r fig.height=14}
kernel_by_factors(log(dataset.with.categories$CRIM+1), "CRIM", "Paired")
```

Again, the density curves for the CRIME variable do classify different populations easily. We can see for example that there is a big difference in the kurtosis for each group in LSTAT. We can see that for high-classes, the index of criminality is centered close to 0, whereas for low classes, it takes higher indexes.

We can easily check this big difference in the median values using boxplots.

```{r}
ggplot(dataset.with.categories, aes(y=CRIM, x=LSTAT.CAT)) + 
  geom_boxplot(outlier.colour="red", outlier.shape=8,
                outlier.size=2) + ylim(c(0,50))
```

We are going to visualize the PCP plot grouping the values by the categorical variable: _RAD.CAT_ (the visualization is in blue for low, green for medium and orange for high): 

```{r}
color_rad_cat=c("blue","green","orange2")[dataset.with.categories$RAD.CAT]
parcoord(dataset.with.categories[,c(1,5,7:8,11,12,14)],col=color_rad_cat,var.label=TRUE)
```
We can see that for high _RAD.CAT_ the variables more affected are CRIM, AGE and DIS. Nevertheless, we will analyze this plot deeper in the outlier section.

Let's plot the andrews curve:

```{r}
library(andrews)
par(mfrow=c(1,1))
andrews(df=as.data.frame(cbind(dataset[,c(1,5,7:8,11,12,14)],dataset.with.categories$CHAS)),clr=8,ymax=4)
```

We have realized that there are a few outliers in the dataset, this is represented as an isolated line.

## Second step

The goal of this step is the estimation of the main characteristics of the quantitative variables. After the aggregation of some continuous variables, our dataset finally contains four continuous variables: *MEDV*, *TAX*, *NOX* and *CRIME*. 

We have to consider that our data come from an homogeneous population. As we have seen in the previous step, exploratory analysis has suggested that we can split data in different groups. Hence, in order to check this assumption we will do inference over data estimating the mean vector, covariance matrix and correlation matrix. Therefore, we will be able to state each group formed belongs to a different population separately.

The dataset is located in the scenario where there are more observations than variables (i.e. $n > p$, where $n$ is the number of observations which the dataset contains and $p$ the number of attributes). Thus, we will use the following expressions to compute the different estimations for a dataset with $n$ observations and $p$ variables:

 - Estimation of $\mu$ as $E[\bar{x}]=\mu$, where $X = ({x}_{1},..., {x}_{p})^{T}$ and $\bar{x}$ is calculated as:

$$
\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i.} =
\begin{pmatrix}
 \bar{x}_{1}\\
 \bar{x}_{2} \\
 \vdots\\
 \bar{x}_{p}
\end{pmatrix}
$$

 - Estimation of $\sum$ as $E[S] = \sum$, where $S$ is the covariance matrix of $x = ({x}_{1},..., {x}_{p})^{T}$ and it is obtained as following:

$$
 S = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i.}-\bar{x})(x_{i.}-\bar{x}) =
\begin{pmatrix}
 {s}_{1}^{2}&{s}_{12}^{2}&...&{s}_{1p}^{2}\\
 {s}_{21}^{2}&{s}_{2}^{2}&...&{s}_{2p}^{2}\\
 \vdots & \ddots & \ddots & \vdots \\
 {s}_{n1}^{2}&{s}_{n2}^{2}&...&{s}_{p}^{2}
\end{pmatrix}
$$

where ${s}_{p}^{2}$ is the sample variance of $x_j$ and ${s}_{jk}^{2}$ is the sample covariance bewteen $x_j$ and $x_k$ with $j\neq	k$.

 - Estimation of correlation matrix of $X = ({x}_{1},..., {x}_{p})^{T}$ through covariance matrix as following:
 
$$
 R = 
\begin{pmatrix}
 1&{r}_{12}&...&{r}_{1p}\\
 {r}_{21}&1&...&{r}_{2p}\\
 \vdots & \ddots & \ddots & \vdots \\
 {r}_{n1}&{r}_{n2}&...&1
\end{pmatrix}
$$

knowing that ${r}_{jk}$ with $j\neq	k$ is computed with expression ${r}_{jk} = \frac{{s}_{jk}}{{s}_{j}{s}_{k}}$ which is the sample correlation coefficient between $x_j$ and $x_k$.


From the conclusions obtained  in the  analysis step, we will compute the previous expressions. We have identified a group formed by the qualitative variable *LSTAT.CAT* and the four quantitative attributes left, as we consider that any quantitative variable could be different for both of the two classes. Therefore, we will be able to see if certain attribute is related with other.

```{r}
X_quan = dataset.with.categories %>% dplyr::select(TAX, MEDV, CRIM, NOX)

#SAMPLE MEAN, COVARIANCE AND CORRELATION FOR QUANTITATIVE VARIABLES
m_quan <- colMeans(X_quan)
S_quan <- cov(X_quan)
R_quan <- cor(X_quan)

# LSTAT - QUATITATIVE VARIABLES
# LSTAT: Two possibles categories 
X_LSTAT_mhc <- X_quan[dataset.with.categories$LSTAT.CAT =="Medium-High-class",]
X_LSTAT_lc <- X_quan[dataset.with.categories$LSTAT.CAT =="Low-class",]

#SAMPLE MEAN, COVARIANCE AND CORRELATION MATRIX FOR LSTAT <= VERY LOW
m_LSTAT_mhc <- colMeans(X_LSTAT_mhc)
S_LSTAT_mhc <- cov(X_LSTAT_mhc)
R_LSTAT_mhc <- cor(X_LSTAT_mhc)

#SAMPLE MEAN, COVARIANCE AND CORRELATION MATRIX FOR LSTAT <= LOW
m_LSTAT_lc <- colMeans(X_LSTAT_lc)
S_LSTAT_lc <- cov(X_LSTAT_lc)
R_LSTAT_lc <- cor(X_LSTAT_lc)
```

### Interpretation of mean vector

```{r}
mean_vector_comp <- rbind(m_quan, m_LSTAT_lc, m_LSTAT_mhc)
rownames(mean_vector_comp) <- c("Total", "Lower Clas", "Medium-High Class")
knitr::kable(mean_vector_comp)
```

According to the plots seen before, we can see that the mean of the observations with "Low-class" is lower than those observations which belong to the other class for the median value of prices. It means that an upper percentage of 

### Interpretation of covariance matrix

```{r}
S_quan
S_LSTAT_mhc
S_LSTAT_lc
```

### Interpretation of correlation matrix

```{r}
R_quan
R_LSTAT_mhc
R_LSTAT_lc
```

## Outliers

The next step is about analyzing the outliers present in the dataset. Then, we are going to use the _Mahalanobis_ distance which plays an important role in outlier detection using the _Minimum Covariance Determinant_ (MCD) which is based on this distance. This functionality is provided by the package _rrcov_. 

First, we are going to calculate the estimators:

```{r}
if (!require(RColorBrewer)) install.packages("RColorBrewer")
library(rrcov)

mcd.estimators <- CovMcd(dataset,alpha=0.85,nsamp="deterministic")
mean.mcd.estimators <- mcd.estimators$center
covariance.mcd.estimators <- mcd.estimators$cov
correlation.mcd.estimators <- cov2cor(covariance.mcd.estimators)

```

The _Mahalanobis_ distance calculated using the MCD procedure is:

```{r}
colors = brewer.pal(n=2, name="Dark2")
n <- nrow(dataset)
p <- ncol(dataset)
x.sq.mah.mcd <- mcd.estimators$mah
var.outliers.mah.mcd <- rep(colors[1], n)
outliers.mah.mcd <- which(x.sq.mah.mcd > qchisq(.99, p))
var.outliers.mah.mcd[outliers.mah.mcd] <- colors[2]

```


## Imput missing data

Checking for NA

```{r}
missingValues=function(data){
  count=0
  a=cbind(lapply(lapply(data, is.na), sum))
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat(as.integer(a[i]), "missing values in column ", i,"\n" )
      count=count+1
    }
  }  
    if(count==0){
      cat("There are No missing values in this dataset")
    }
}

missingValues(dataset)
```

Impute NA

```{r}
#With the mean
clean_data_mean=dataset
index=which(is.na(dataset$RM))
clean_data_mean$RM[index]=mean(clean_data_mean$RM,na.rm = TRUE)
clean_data_mean$RM[index]

#Using mice library
library(mice)
set.seed(1234)
methods(mice)
imputation=mice(dataset,m=5,method="pmm",maxit = 20)

#Check each prediction
imputation$imp$RM

#We want to check something close to the mean value 6.288 so we are going to take the first prediction, because it has an error lower than 0.5 on each boundary
summary(dataset$RM)

for(i in 1:5){
    print(imputation$imp$RM[i]-clean_data_mean$RM[index])
}

clean_data=complete(imputation,1)
```



