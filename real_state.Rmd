---
title: "Real_state"
author: "Ignacio Almodóvar, Luis Rodríguez, Javier Muñoz"
date: "12/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First of all, we are going to read the Boston house prices dataset file and show the summary and the structure:

```{r}
library(ggplot2)
library(dplyr)
library(MASS)
dataset <- read.csv("Dataset.csv")
```

Variables explanation

Attribute Information:

- CRIM: This is a continuous variable that measures the crime per capital rate by town.
- ZN: Continuous variable that indicates the proportion of residential land zoned for lots over 25,000 sq.ft. There are not many different values in this category. Could be also considered a categorical variable.
- INDUS: Proportion of non-retail business acres per town. Continuous variable.
- CHAS: It is a binary variable referred to Charles River (= 1 if tract bounds river; 0 otherwise).
- NOX: Continuous variable that measures nitric oxides concentration (parts per 10 million).
- RM: Measures the average number of rooms per dwelling. Continuous variable.
- AGE: Continuous variable that measures the proportion of owner-occupied units built prior to 1940.
- DIS: Continuous variable that indicates the weighted distances to five Boston employment centers.
- RAD: Index of accessibility to radial highways. Larger index denotes better accessibility.
- TAX: Continuous variable full-value property-tax rate per $10,000.
- PTRATIO: Continuous variable that measures the proportion of pupil-teacher by town.
- B: Is a continuous variable based on the formula 1000(Bk - 0.63)^2, where BK is the proportion of black people.
- LSTAT: Continuous variable that measures the proportion of adults without, some high school education and proportion of male workers classified as laborers.
- MEDV: Continuous variable with the median value price of owner-occupied homes in $1000's


Next step is to convert the type of the categorical variable _CHAS_ into factor and create a few categorical variables from suitable quantitative ones:

- _RAD.CAT_: low, medium and high
- _RM.CAT_: low, medium and high
- _INDUS.CAT_: very low, low, high and very high
- _LSTAT.CAT_: very low, low, high and very high
- _ZN.CAT_: very low, low, high and very high

```{r}

dataset$CHAS <- as.factor(dataset$CHAS)

factor_rad_values <- sort(unique(dataset$RAD))
delimiter_rad_index <- length(factor_rad_values)/3
lower_delimiter_rad_value <- factor_rad_values[round(delimiter_rad_index)]
upper_delimiter_rad_value <- factor_rad_values[round(2*delimiter_rad_index)]

dataset$RAD.CAT <- cut(dataset$RAD, 
    breaks = c(-Inf, lower_delimiter_rad_value, upper_delimiter_rad_value, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

dataset$RM.CAT <- cut(dataset$RM, 
    breaks = c(-Inf, 4, 6, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

factor_indus_values <- sort(unique(dataset$INDUS))
delimiter_indus_index <- length(factor_indus_values)/4
lower_delimiter_indus_value <- factor_indus_values[round(delimiter_indus_index)]
mid_delimiter_indus_value <- factor_indus_values[round(2*delimiter_indus_index)]
upper_delimiter_indus_value <- factor_indus_values[round(3*delimiter_indus_index)]

dataset$INDUS.CAT <- cut(dataset$INDUS, 
    breaks = c(-Inf, lower_delimiter_indus_value, mid_delimiter_indus_value, upper_delimiter_indus_value, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

dataset$LSTAT.CAT <- cut(dataset$LSTAT, 
    breaks = c(-Inf, 25, 50, 75, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

dataset$ZN.CAT <- cut(dataset$ZN, 
    breaks = c(-Inf, 25, 50, 75, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

str(dataset)

```



I'm going to try to find differences in shapes between the different levels of a predictor and the variable that we want to predict, the average price.



```{r}
pairs(data=dataset,~CRIM+ZN+INDUS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT+MEDV,pch=19,col=dataset$CHAS)
```

Within this plot we can see that there is a linear relationship between (NOX,DIS) and (LSTAT,MEDV)

Now, lets try to find groups through scatter plots.
```{r}
ggplot(data=dataset,aes(x=NOX,y=DIS))+geom_point(aes(colour=ZN.CAT))

ggplot(data=dataset,aes(y=MEDV,x=LSTAT))+geom_point(aes(colour=LSTAT.CAT))
```

Density curves

```{r}
#Given river
dataset %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=CHAS))

#Given rooms
dataset %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=RM.CAT,fill=RM.CAT))

dataset %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=LSTAT.CAT))

dataset %>% ggplot(aes(x=(NOX))) + geom_density(aes(color=ZN.CAT))
```

PCP plot

```{r}
color_rad_cat=c("blue","green","orange2")[dataset$RAD.CAT]
parcoord(dataset[,c(1,5,7:8,11,12,14)],col=color_rad_cat,var.label=TRUE)
```
We can see that for high RAD.CAT the variables more affected are CRIM, AGE and DIS

ANDREWS curve

```{r}
library(andrews)
par(mfrow=c(1,1))
andrews(df=as.data.frame(cbind(dataset[,c(1,5,7:8,11,12,14)],dataset$CHAS)),clr=8,ymax=4)
```

<<<<<<< Updated upstream
###############################3
=======
### Interpretation of correlation matrix

```{r}
knitr::kable(R_quan, caption="Correlation Matrix: total classes")
knitr::kable(R_LSTAT_lc, caption="Correlation Matrix: lower class")
knitr::kable(R_LSTAT_mhc, caption="Correlation Matrix: medium high classes")
```


## Imputation of missing values
>>>>>>> Stashed changes

Missing values are a very frequent problem that appears in almost every dataset. Therefore, it is important to know how to deal with them.

There are several ways to impute missing values. One of the easiest is based on deleting all the observations that contains missing values. This could work if your dataset is very long and you do not have many NA, otherwise you will be deleting too much information. However, in practice, this is not a very common solution to deal with missing values.

Another very popular solution is to replace all the missing values with the sample mean, sample median or sample mode, as these values are "close" to what could be expected for them. Nevertheless, this practice could be very dangerous in some cases. Imagine for example a binary variable, if we used the mean value to replace NAs it will give a value in between 1-0, which is not going to be a useful replacement.

There are other methods to impute missing values that are based on predictions, which tempt to be the most efficient. Therefore, we will use them if needed. 

First of all, we are going to build a function that summarizes if there are any NA in our dataset. 

```{r}
missingValues=function(data){
  count=0
  a=cbind(lapply(lapply(data, is.na), sum))
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat(as.integer(a[i]), "missing values in column ", i,"\n" )
      count=count+1
    }
  }  
    if(count==0){
      cat("There are No missing values in this dataset")
    }
}

missingValues(dataset)
```

We found that there are 5 missing values in the variable RM. In order to impute them, we are going to see different methods.

First of all we are going to see how would it look to replace our missing values with the mean value of the variable.

<<<<<<< Updated upstream
```{r}
#With the mean
=======
```{r, warning=FALSE}
>>>>>>> Stashed changes
clean_data_mean=dataset
index=which(is.na(dataset$RM))
clean_data_mean$RM[index]=mean(clean_data_mean$RM,na.rm = TRUE)
```

As expected we replace all the missing value as the mean value of the column. However, as we mentioned before, using this method is very dangerous. Therefore, we are going to use some functions provided by the library "mice" that predicts the missing values based on the rest of the information contained in the dataset. 

```{r, warning=FALSE,results=FALSE}
set.seed(1234)
#methods(mice)
imputation=mice(dataset,m=5,method="pmm",maxit = 20)
```

As we specified that we wanted 5 predictions, the mice function will gives us 5 different predictions.

```{r, warning=FALSE}
imputation$imp$RM
```

Once we have the predictions we need a method to select which one is the better. It won't be a problem to use any of the ones obtained, however there are some useful tips that could help to choose the best one. As the mean value in our dataset is not very far away from the majority of the observations we really don't want any prediction to be very far away from the mean value. Therefore we are going to compute the errors between each prediction and the mean in order to see which one has less error.

```{r, warning=FALSE}
for(i in 1:5){
    print(imputation$imp$RM[i]-clean_data_mean$RM[index])
}
```

Looks like in this case, the second prediction is the best one, as the higher value for the error is 0.25. Therefore, we choose this prediction to compute our missing values.

```{r, warning=FALSE}
clean_data=complete(imputation,1)
```


