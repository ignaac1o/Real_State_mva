title: "Assignment - Midterm - Boston Housing"
subtitle: "Multivariate Analysis"
author: "Ignacio Almodóvar, Luis Rodríguez, Javier Muñoz"
date: "12/12/2021"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{mathtools}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First of all, we are going to read the Boston house prices dataset file and show the summary and the structure:

```{r}
library(ggplot2)
library(dplyr)
library(MASS)
dataset <- read.csv("Dataset.csv")

summary(dataset)
str(dataset)

```

## Variables explanation

Attribute Information:

- CRIM: This is a continuous variable that measures the crime per capital rate by town.
- ZN: Continuous variable that indicates the proportion of residential land zoned for lots over 25,000 sq.ft. There are not many different values in this category. Could be also considered a categorical variable.
- INDUS: Proportion of non-retail business acres per town. Continuous variable.
- CHAS: It is a binary variable referred to Charles River (= 1 if tract bounds river; 0 otherwise).
- NOX: Continuous variable that measures nitric oxides concentration (parts per 10 million).
- RM: Measures the average number of rooms per dwelling. Continuous variable.
- AGE: Continuous variable that measures the proportion of owner-occupied units built prior to 1940.
- DIS: Continuous variable that indicates the weighted distances to five Boston employment centers.
- RAD: Index of accessibility to radial highways. Larger index denotes better accessibility.
- TAX: Continuous variable full-value property-tax rate per $10,000.
- PTRATIO: Continuous variable that measures the proportion of pupil-teacher by town.
- B: Is a continuous variable based on the formula 1000(Bk - 0.63)^2, where BK is the proportion of black people.
- LSTAT: Continuous variable that measures the proportion of adults without, some high school education and proportion of male workers classified as laborers.
- MEDV: Continuous variable with the median value price of owner-occupied homes in $1000's


Next step is to convert the type of the categorical variable _CHAS_ into factor and create a few categorical variables from suitable quantitative ones:

- _RAD.CAT_: low, medium and high
- _RM.CAT_: low, medium and high
- _INDUS.CAT_: very low, low, high and very high
- _LSTAT.CAT_: very low, low, high and very high
- _ZN.CAT_: very low, low, high and very high

```{r}

dataset.with.categories <- dataset

dataset.with.categories$CHAS <- factor(dataset.with.categories$CHAS, labels=c("no bounds", "bounds"))

factor_rad_values <- sort(unique(dataset.with.categories$RAD))
delimiter_rad_index <- length(factor_rad_values)/3
lower_delimiter_rad_value <- factor_rad_values[round(delimiter_rad_index)]
upper_delimiter_rad_value <- factor_rad_values[round(2*delimiter_rad_index)]

dataset.with.categories$RAD.CAT <- cut(dataset.with.categories$RAD, 
    breaks = c(-Inf, lower_delimiter_rad_value, upper_delimiter_rad_value, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

dataset.with.categories$RM.CAT <- cut(dataset.with.categories$RM, 
    breaks = c(-Inf, 4, 6, Inf), 
    labels = c('low', 'medium', 'high'),
    right = FALSE)

factor_indus_values <- sort(unique(dataset.with.categories$INDUS))
delimiter_indus_index <- length(factor_indus_values)/4
lower_delimiter_indus_value <- factor_indus_values[round(delimiter_indus_index)]
mid_delimiter_indus_value <- factor_indus_values[round(2*delimiter_indus_index)]
upper_delimiter_indus_value <- factor_indus_values[round(3*delimiter_indus_index)]

dataset.with.categories$INDUS.CAT <- cut(dataset.with.categories$INDUS, 
    breaks = c(-Inf, lower_delimiter_indus_value, mid_delimiter_indus_value, upper_delimiter_indus_value, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

dataset.with.categories$LSTAT.CAT <- cut(dataset.with.categories$LSTAT, 
    breaks = c(-Inf, 25, 50, 75, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

dataset.with.categories$ZN.CAT <- cut(dataset.with.categories$ZN, 
    breaks = c(-Inf, 25, 50, 75, Inf), 
    labels = c('very low', 'low', 'high', 'very high'),
    right = FALSE)

str(dataset.with.categories)

```

We are going to define a method to generate the kernel plots: withouth any categorical variable and groupped by one qualitative:

```{r}

if (!require(RColorBrewer)) install.packages("RColorBrewer")
library(RColorBrewer)

kernel_by_factors <- function(x, x_name, type_color) {
  factors <- Filter(is.factor, dataset)
  num_vars <- ncol(factors)
  mod <- num_vars %% 2 
  
  colors = brewer.pal(n = num_vars + 1, name=type_color)

  par(mfrow=c(num_vars+mod+1,2), mar=c(2,2,2,2))
  plot(density(x,kernel="gaussian"),
   ylab="Density",
   main=paste0("Kernel density of ", x_name),
   xlab=x_name,
   col=colors[1],
   lwd=5)
  
  index_factor <- 1
  for(factor in factors) {
    plot_kernel_charts(x, factor[!is.na(factor)], x_name, names(factors[index_factor]), colors)
    index_factor = index_factor + 1
  }
}

plot_kernel_charts <- function(x, cat, x_name, cat_name, colors) {
  cat_values <- levels(cat)
  num_colors <- length(cat_values)
  
  min_x <<- c()
  max_x <<- c()
  min_y <<- c()
  max_y <<- c()
  
  densities <<- list()
  
  for(cat_value in cat_values)
    generate_kernel_by_cat_value(x, cat, cat_value)
  
  min_x <- min(min_x)
  max_x <- max(max_x)
  min_y <- min(min_y)
  max_y <- max(max_y)
  
  plot(c(min_x,max_x),c(min_y,max_y),
       xlab=x_name,
       ylab="Density",
       main=paste0("Kernel: ", x_name, " in terms of ", cat_name),
       type="n")
  legend(x="topright", legend=c(cat_values),col=colors[-1], lty=1, cex=0.8,
     box.lty=0)
  i <- 2
  for(densitiy_by_cat in densities) {
    lines(densitiy_by_cat$x, densitiy_by_cat$y, col=colors[i], lwd=1)
    i = i + 1
  }
}

generate_kernel_by_cat_value <- function(x, cat, cat_value){
  if(length(cat[cat==cat_value]) > 2){
    d_cat_value <- density(x[cat==cat_value], kernel="gaussian")
    min_x <<- c(min_x, min(d_cat_value$x))
    max_x <<- c(max_x, max(d_cat_value$x))
    min_y <<- c(min_y, min(d_cat_value$y))
    max_y <<- c(max_y, max(d_cat_value$y))
    
    densities <<- append(densities, list(d_cat_value))
  }
}

```

Calling the method above with the parameters of interest: _TAX_, _MEDV_, _CRIM_ and _NOX_

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$TAX, "TAX", "Set1")
```

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$MEDV, "MEDV", "Dark2")
```

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$CRIM, "CRIM", "Paired")
```

```{r fig.height=14}
kernel_by_factors(dataset.with.categories$NOX, "NOX", "Accent")
```

We are going to visualize the matrix correlation between each variable:

```{r fig.width=14, fig.height=14}
if (!require(ggplot2)) install.packages("ggplot2")
if (!require(GGally)) install.packages("GGally")
library(ggplot2)
library(GGally)

is.not.fact <- sapply(dataset.with.categories, Negate(is.factor))
dataset.no.factors <- dataset.with.categories[,is.not.fact]
ggpairs(dataset.no.factors, title="Pairwise variable correlation")

```


```{r}
pairs(data=dataset.with.categories,~CRIM+ZN+INDUS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT+MEDV,pch=19,col=dataset$CHAS)
```

Within this plot we can see that there is a linear relationship between (NOX,DIS) and (LSTAT,MEDV)

Now, lets try to find groups through scatter plots.
```{r}
ggplot(data=dataset.with.categories,aes(x=NOX,y=DIS))+geom_point(aes(colour=ZN.CAT))

ggplot(data=dataset.with.categories,aes(y=MEDV,x=LSTAT))+geom_point(aes(colour=LSTAT.CAT))
```

Density curves

```{r}
#Given river
dataset.with.categories %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=CHAS))

#Given rooms
dataset.with.categories %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=RM.CAT,fill=RM.CAT))

dataset.with.categories %>% ggplot(aes(x=MEDV)) + geom_density(aes(color=LSTAT.CAT))

dataset.with.categories %>% ggplot(aes(x=(NOX))) + geom_density(aes(color=ZN.CAT))
```

### PCP plot

```{r}
color_rad_cat=c("blue","green","orange2")[dataset.with.categories$RAD.CAT]
parcoord(dataset.with.categories[,c(1,5,7:8,11,12,14)],col=color_rad_cat,var.label=TRUE)
```
We can see that for high RAD.CAT the variables more affected are CRIM, AGE and DIS

ANDREWS curve

```{r}
library(andrews)
par(mfrow=c(1,1))
andrews(df=as.data.frame(cbind(dataset[,c(1,5,7:8,11,12,14)],dataset.with.categories$CHAS)),clr=8,ymax=4)
```


## Second step

The goal of this step is the estimation of the main characteristics of the quantitative variables. After the transformation of some of quantitatives attributes in cathegorical variables, our dataset contains four continuous variables: *MEDV*, *TAX*, *NOX* and *CRIME*. 

We have to consider our data come from an homogeneous population. As we have seen in the previous step, exploratory analysis has suggested that we can divide data in different groups. Hence, in order to check this assumption we will do inference over data estimating the mean vector, covariance matrix and correlation matrix. Therefore, we will be able to state each group formed belongs to a different population separately.

The dataset is located in the scenario where there are more observations than variables (i.e. $n > p$, where $n$ is the number of observations which the dataset contains and $p$ the number of attributes). Thus, we will use the following expressions to compute the different estimations for a dataset with $n$ observations and $p$ variables:

 - Estimation of $\mu$ as $E[\bar{x}]=\mu$, where $X = ({x}_{1},..., {x}_{p})^{T}$ and $\bar{x}$ is calculated as:

$$
\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_{i.} =
\begin{pmatrix}
 \bar{x}_{1}\\
 \bar{x}_{2} \\
 \vdots\\
 \bar{x}_{p}
\end{pmatrix}$$

 - Estimation of $\sum$ as $E[S] = \sum$, where $S$ is the covariance matrix of $x = ({x}_{1},..., {x}_{p})^{T}$ and it is obtained as following:

$$
 S = \frac{1}{n-1}\sum_{i=1}^{n}(x_{i.}-\bar{x})(x_{i.}-\bar{x}) =
\begin{pmatrix}
 {s}_{1}^{2}&{s}_{12}^{2}&...&{s}_{1p}^{2}\\
 {s}_{21}^{2}&{s}_{2}^{2}&...&{s}_{2p}^{2}\\
 \vdots & \ddots & \ddots & \vdots \\
 {s}_{n1}^{2}&{s}_{n2}^{2}&...&{s}_{p}^{2}
\end{pmatrix}$$

where ${s}_{p}^{2}$ is the sample variance of $x_j$ and ${s}_{jk}^{2}$ is the sample covariance bewteen $x_j$ and $x_k$ with $j\neq	k$.

 - Estimation of correlation matrix of $X = ({x}_{1},..., {x}_{p})^{T}$ through covariance matrix as following:
 
$$
 R = 
\begin{pmatrix}
 1&{r}_{12}&...&{r}_{1p}\\
 {r}_{21}&1&...&{r}_{2p}\\
 \vdots & \ddots & \ddots & \vdots \\
 {r}_{n1}&{r}_{n2}&...&1
\end{pmatrix}
$$

knowing that ${r}_{jk}$ with $j\neq	k$ is computed with expression ${r}_{jk} = \frac{{s}_{jk}}{{s}_{j}{s}_{k}}$ which is the sample correlation coefficient between $x_j$ and $x_k$.


In view of the conclusions which we have obtained in analysis step, we will compute the previous expressions. We have identified a group formed by the cualitative variable *LSTAT.CAT* and the four quantitative attributes left, as we consider any quantitative variable could be different for both of the two classes. Therefore, we will be able to see if certain attribute is related with other.


```{r}
if (!require("corrplot")) install.packages("corrplot")
library("corrplot")
if (!require("dplyr")) install.packages("dpylr")
library(dplyr)
#####################################################################

X <- read.csv("Dataset.csv")

# Select quantitative variables
X_quan = X %>% select(TAX, MEDV, CRIM, NOX)

#SAMPLE MEAN, COVARIANCE AND CORRELATION FOR QUANTITATIVE VARIABLES
m_quan <- colMeans(X_quan)
S_quan <- cov(X_quan)
R_quan <- cor(X_quan)

# LSTAT - QUATITATIVE VARIABLES
# LSTAT: Two possibles categories 

X_LSTAT_verylow <- X_quan[X$LSTAT <= 25,]
X_LSTAT_low <- X_quan[X$LSTAT >= 25,]

#SAMPLE MEAN, COVARIANCE AND CORRELATION MATRIX FOR LSTAT <= VERY LOW
m_LSTAT_verylow <- colMeans(X_LSTAT_verylow)
S_LSTAT_verylow <- cov(X_LSTAT_verylow)
R_LSTAT_verylow <- cor(X_LSTAT_verylow)

#SAMPLE MEAN, COVARIANCE AND CORRELATION MATRIX FOR LSTAT <= LOW
m_LSTAT_low <- colMeans(X_LSTAT_low)
S_LSTAT_low <- cov(X_LSTAT_low)
R_LSTAT_low <- cor(X_LSTAT_low)

```
## Interpretation of mean vector

```{r}
m_quan
m_LSTAT_low
m_LSTAT_verylow
```

We can see the mean of the observations in which class *LSTAT* is 0 is lower than those observations which belong to the other class to the variable *MEDV*. It means that an upper percentage of 


## Outliers

The next step is about analyzing the outliers present in the dataset. Then, we are going to use the _Mahalanobis_ distance which plays an important role in outlier detection using the _Minimum Covariance Determinant_ (MCD) which is based on this distance. This functionality is provided by the package _rrcov_. 

First, we are going to calculate the estimators:

```{r}
if (!require(RColorBrewer)) install.packages("RColorBrewer")
library(rrcov)

mcd.estimators <- CovMcd(dataset,alpha=0.85,nsamp="deterministic")
mean.mcd.estimators <- mcd.estimators$center
covariance.mcd.estimators <- mcd.estimators$cov
correlation.mcd.estimators <- cov2cor(covariance.mcd.estimators)

```

The _Mahalanobis_ distance calculated using the MCD procedure is:

```{r}
colors = brewer.pal(n=2, name="Dark2")
n <- nrow(dataset)
p <- ncol(dataset)
x.sq.mah.mcd <- mcd.estimators$mah
var.outliers.mah.mcd <- rep(colors[1], n)
outliers.mah.mcd <- which(x.sq.mah.mcd > qchisq(.99, p))
var.outliers.mah.mcd[outliers.mah.mcd] <- colors[2]

```


## Imput missing data

Checking for NA

```{r}
missingValues=function(data){
  count=0
  a=cbind(lapply(lapply(data, is.na), sum))
  for(i in 1:ncol(data)){
    if(a[i]!=0){
      cat(as.integer(a[i]), "missing values in column ", i,"\n" )
      count=count+1
    }
  }  
    if(count==0){
      cat("There are No missing values in this dataset")
    }
}

missingValues(dataset)
```

Impute NA

```{r}
#With the mean
clean_data_mean=dataset
index=which(is.na(dataset$RM))
clean_data_mean$RM[index]=mean(clean_data_mean$RM,na.rm = TRUE)
clean_data_mean$RM[index]

#Using mice library
library(mice)
set.seed(1234)
methods(mice)
imputation=mice(dataset,m=5,method="pmm",maxit = 20)

#Check each prediction
imputation$imp$RM

#We want to check something close to the mean value 6.288 so we are going to take the first prediction, because it has an error lower than 0.5 on each boundary
summary(dataset$RM)

for(i in 1:5){
    print(imputation$imp$RM[i]-clean_data_mean$RM[index])
}

clean_data=complete(imputation,1)
```



